cv : True
crf_lr : 0.1
pred_dir : bert
data_type : default
fold : 4
use_pos_tag : 1
do_pred : 1
update_every : 1
status : train
use_bert : 1
only_bert : 1
fix_bert_epoch : 0
after_bert : mlp
msg : 11266
train_clip : True
device : 0
debug : 0
gpumm : False
see_convergence : False
see_param : False
test_batch : 8
seed : 1080956
test_train : False
number_normalized : 3
lexicon_name : lk
use_pytorch_dropout : 0
char_min_freq : 1
bigram_min_freq : 1
lattice_min_freq : 1
only_train_min_freq : True
only_lexicon_in_train : False
word_min_freq : 1
early_stop : 20
epoch : 5
batch : 16
optim : adam
lr : 0.0006
bert_lr_rate : 0.05
embed_lr_rate : 1.3
momentum : 0.9
init : uniform
self_supervised : False
weight_decay : 0.03
norm_embed : True
norm_lattice_embed : True
warmup : 0.1
model : transformer
lattice : 1
use_bigram : 0
hidden : -1
ff : 3
layer : 1
head : 8
head_dim : 20
scaled : False
ff_activate : relu
k_proj : False
q_proj : True
v_proj : True
r_proj : True
attn_ff : False
use_abs_pos : false
use_rel_pos : true
rel_pos_shared : True
add_pos : False
learn_pos : False
pos_norm : False
rel_pos_init : 1
four_pos_shared : True
four_pos_fusion : ff_two
four_pos_fusion_shared : True
pre : 
post : n
embed_dropout_before_pos : False
embed_dropout : 0.5
gaz_dropout : 0.5
output_dropout : 0.3
pre_dropout : 0.5
post_dropout : 0.3
ff_dropout : 0.15
ff_dropout_2 : 0.15
attn_dropout : 0
embed_dropout_pos : 0
abs_pos_fusion_func : nonlinear_add
dataset : aicup
Read cache from cache/defaultk4_cv:True_trainClip:Truebgminfreq_1char_min_freq_1word_min_freq_1only_train_min_freqTruenumber_norm3load_dataset_seed100.
用的词表的路径:/home/dy/flat-chinese-ner/data/pretrain/sgns.merge.word
Read cache from cache/lk.
Read cache from cache/defaultk4_cv:True_lattice_only_train:False_trainClip:True_norm_num:3char_min_freq1bigram_min_freq1word_min_freq1only_train_min_freqTruenumber_norm3lexicon_lkload_dataset_seed_100.
train:2852
train 最长的句子是:['那', '就', '是', '发', '炎', '指', '数', '啊', '。', '啊', '你', '如', '果', '这', '边', '比', '较', '好', '，', '你', '这', '边', '下', '来', '。', '对', '啊', '。', '要', '不', '然', '就', '追', '踪', '看', '看', '。', '啊', '所', '以', '说', '那', '个', '没', '有', '做', '了', '。', '没', '有', '做', '了', '。', '这', '个', '年', '度', '检', '查', '没', '做', '，', '想', '说', '再', '看', '看', '啊', '。', '啊', '血', '压', '药', '这', '样', '子', '吃', '有', '没', '有', '要', '调', '整', '？', '我', '就', '暂', '时', '这', '样', '吃', '。', '暂', '时', '这', '样', '。', '最', '近', '比', '较', '常', '运', '动', '。', '啊', '刚', '来', '是', '0', '0', '0', '和', '0', '0', '。', '早', '上', '的', '时', '候', '，', '的', '指', '数', '。']
train max_seq_len:126
train max_lex_num:131
train max_seq_lex:254
dev 最长的句子是:['我', '是', '说', '肾', '脏', 'O', 'K', '，', '啊', '因', '为', '有', '点', '轻', '微', '的', '脂', '肪', '肝', '。', '轻', '微', '的', '脂', '肪', '肝', '。', '对', '。', '这', '边', '…', '…', '我', '看', '一', '下', '。', '0', '.', '0', '公', '分', '，', '这', '颗', '息', '肉', '，', '大', '概', '0', '.', '0', '公', '分', '，', '啊', '不', '过', '这', '就', '是', '要', '追', '踪', '。', '就', '是', '注', '意', '一', '下', '它', '有', '没', '有', '变', '大', '。', '在', '一', '公', '分', '以', '内', '的', '话', '，', '我', '们', '大', '概', '就', '是', '追', '踪', '就', '可', '以', '了', '。', '之', '前', '是', '多', '大', '？', '嗯', '…', '…', '来', '看', '看', '。', '0', '0', '0', '0', '年', '喔', '，', '好', '像', '是', '，']
dev max_seq_len:126
dev max_lex_num:83
dev max_seq_lex:208
aicup_dev 最长的句子是:['我', '增', '加', '的', '时', '候', '是', '用', '了', '不', '好', '的', '东', '西', '，', '我', '去', '找', '了', '一', '些', '类', '固', '醇', '。', '类', '固', '醇', '喔', '。', '对', '，', '阿', '你', '有', '搭', '配', '吃', '什', '么', '高', '蛋', '白', '？', '有', '有', '有', '，', '然', '后', '加', '重', '训', '，', '就', '是', '两', '个', '月', '胖', '一', '公', '斤', '我', '觉', '得', '没', '什', '么', '用', '。', '应', '该', '是', '增', '加', '肌', '肉', '啦', '。', '对', '，', '可', '是', '还', '是', '胖', '不', '了', '，', '很', '想', '去', '打', '脂', '肪', '。', '不', '要', '，', '你', '这', '样', '还', '好', '啦', '。', '我', '那', '个', '时', '候', '去', '测', '体', '脂', '肪', '，', '我', '体', '脂', '肪', '只', '有', '0', '。']
aicup_dev max_seq_len:126
aicup_dev max_lex_num:86
aicup_dev max_seq_lex:212
raw_chars:['喔', '她', '现', '住', '在', '花', '莲', '。', '对', '，', '不', '过', '因', '为', '我', '姑', '丈', '是', '非', '洲', '人', '啦', '，', '所', '以', '说', '他', '有', '时', '候', '会', '回', '来', '。', '喔', '难', '怪', '。', '嗯', '嗯', 'O', 'K', '。', '好', '，', '那', '我', '们', '就', '六', '月', '二', '十', '八', '号', '。', '可', '以', '啊', '可', '以', '啊', '。', '好', 'O', 'K', '。', '谢', '谢', '谢', '谢', '。', '啊', '你', '这', '一', '个', '月', '还', '好', '吗', '？', '小', '华', '喔', '？', '是', '。', '哦', '所', '以', '你', '选', '择', '，', '小', '花', '喔', '？', '你', '看', '这', '就', '是', '人', '的', '惰', '性', '。', '没', '关', '系', '，', '会', '很', '奇', '怪', '。', '人', '的', '惰', '性', '，']
lexicons:[[3, 4, '住在'], [5, 6, '花莲'], [10, 11, '不过'], [12, 13, '因为'], [15, 16, '姑丈'], [17, 18, '是非'], [18, 19, '非洲'], [18, 20, '非洲人'], [23, 24, '所以'], [27, 28, '有时'], [27, 29, '有时候'], [28, 29, '时候'], [28, 30, '时候会'], [29, 30, '候会'], [31, 32, '回来'], [35, 36, '难怪'], [38, 39, '嗯嗯'], [40, 41, 'OK'], [46, 47, '我们'], [49, 50, '六月'], [51, 52, '二十'], [51, 53, '二十八'], [52, 53, '十八'], [56, 57, '可以'], [59, 60, '可以'], [64, 65, 'OK'], [67, 68, '谢谢'], [68, 69, '谢谢'], [69, 70, '谢谢'], [75, 76, '一个'], [78, 79, '还好'], [82, 83, '小华'], [89, 90, '所以'], [92, 93, '选择'], [95, 96, '小花'], [102, 103, '就是'], [106, 107, '惰性'], [109, 110, '没关'], [109, 111, '没关系'], [110, 111, '关系'], [115, 116, '奇怪'], [120, 121, '惰性']]
lattice:[34, 455, 96, 302, 38, 759, 3466, 2, 17, 3, 16, 97, 48, 37, 7, 1189, 5678, 4, 727, 3137, 127, 52, 3, 40, 20, 41, 23, 5, 49, 72, 18, 148, 46, 2, 34, 750, 539, 2, 30, 30, 121, 126, 2, 9, 3, 15, 7, 39, 6, 335, 71, 240, 211, 408, 222, 2, 33, 20, 32, 33, 20, 32, 2, 9, 121, 126, 2, 284, 284, 284, 284, 2, 32, 11, 12, 13, 14, 71, 29, 9, 58, 10, 170, 2563, 34, 10, 4, 2, 260, 40, 20, 11, 1008, 1389, 3, 170, 759, 34, 10, 11, 57, 12, 6, 4, 127, 8, 3936, 157, 2, 22, 181, 193, 3, 18, 76, 961, 539, 2, 127, 8, 3936, 157, 3, 1223, 3683, 346, 51, 5679, 2993, 3682, 5680, 45, 216, 225, 73, 594, 595, 450, 2269, 174, 129, 50, 1235, 694, 2341, 1371, 66, 66, 129, 516, 516, 516, 91, 172, 3467, 45, 1430, 3937, 26, 3938, 416, 417, 210, 1209, 3938]
raw_lattice:['喔', '她', '现', '住', '在', '花', '莲', '。', '对', '，', '不', '过', '因', '为', '我', '姑', '丈', '是', '非', '洲', '人', '啦', '，', '所', '以', '说', '他', '有', '时', '候', '会', '回', '来', '。', '喔', '难', '怪', '。', '嗯', '嗯', 'O', 'K', '。', '好', '，', '那', '我', '们', '就', '六', '月', '二', '十', '八', '号', '。', '可', '以', '啊', '可', '以', '啊', '。', '好', 'O', 'K', '。', '谢', '谢', '谢', '谢', '。', '啊', '你', '这', '一', '个', '月', '还', '好', '吗', '？', '小', '华', '喔', '？', '是', '。', '哦', '所', '以', '你', '选', '择', '，', '小', '花', '喔', '？', '你', '看', '这', '就', '是', '人', '的', '惰', '性', '。', '没', '关', '系', '，', '会', '很', '奇', '怪', '。', '人', '的', '惰', '性', '，', '住在', '花莲', '不过', '因为', '姑丈', '是非', '非洲', '非洲人', '所以', '有时', '有时候', '时候', '时候会', '候会', '回来', '难怪', '嗯嗯', 'OK', '我们', '六月', '二十', '二十八', '十八', '可以', '可以', 'OK', '谢谢', '谢谢', '谢谢', '一个', '还好', '小华', '所以', '选择', '小花', '就是', '惰性', '没关', '没关系', '关系', '奇怪', '惰性']
lex_s:[3, 5, 10, 12, 15, 17, 18, 18, 23, 27, 27, 28, 28, 29, 31, 35, 38, 40, 46, 49, 51, 51, 52, 56, 59, 64, 67, 68, 69, 75, 78, 82, 89, 92, 95, 102, 106, 109, 109, 110, 115, 120]
lex_e:[4, 6, 11, 13, 16, 18, 19, 20, 24, 28, 29, 29, 30, 30, 32, 36, 39, 41, 47, 50, 52, 53, 53, 57, 60, 65, 68, 69, 70, 76, 79, 83, 90, 93, 96, 103, 107, 110, 111, 111, 116, 121]
pos_s:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 3, 5, 10, 12, 15, 17, 18, 18, 23, 27, 27, 28, 28, 29, 31, 35, 38, 40, 46, 49, 51, 51, 52, 56, 59, 64, 67, 68, 69, 75, 78, 82, 89, 92, 95, 102, 106, 109, 109, 110, 115, 120]
pos_e:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 4, 6, 11, 13, 16, 18, 19, 20, 24, 28, 29, 29, 30, 30, 32, 36, 39, 41, 47, 50, 52, 53, 53, 57, 60, 65, 68, 69, 70, 76, 79, 83, 90, 93, 96, 103, 107, 110, 111, 111, 116, 121]
embedding:torch.Size([2319, 50])
norm embedding
embedding:torch.Size([14239, 50])
norm lattice embedding
loading vocabulary file /home/dy/.fastNLP/embedding/bert-chinese-wwm/vocab.txt
Load pre-trained BERT parameters from file /home/dy/.fastNLP/embedding/bert-chinese-wwm/chinese_wwm_pytorch.bin.
Start to generate word pieces for word.
Found(Or segment into word pieces) 15797 words out of 16105.
bert_embedding.model.encoder.embeddings.word_embeddings.weight:torch.Size([4559, 768])
bert_embedding.model.encoder.embeddings.position_embeddings.weight:torch.Size([512, 768])
bert_embedding.model.encoder.embeddings.token_type_embeddings.weight:torch.Size([2, 768])
bert_embedding.model.encoder.embeddings.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.embeddings.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.0.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.0.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.0.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.0.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.0.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.0.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.0.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.0.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.0.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.0.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.0.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.0.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.0.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.0.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.0.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.0.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.1.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.1.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.1.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.1.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.1.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.1.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.1.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.1.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.1.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.1.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.1.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.1.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.1.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.1.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.1.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.1.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.2.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.2.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.2.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.2.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.2.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.2.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.2.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.2.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.2.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.2.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.2.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.2.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.2.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.2.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.2.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.2.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.3.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.3.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.3.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.3.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.3.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.3.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.3.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.3.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.3.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.3.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.3.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.3.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.3.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.3.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.3.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.3.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.4.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.4.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.4.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.4.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.4.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.4.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.4.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.4.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.4.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.4.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.4.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.4.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.4.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.4.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.4.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.4.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.5.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.5.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.5.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.5.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.5.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.5.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.5.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.5.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.5.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.5.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.5.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.5.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.5.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.5.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.5.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.5.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.6.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.6.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.6.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.6.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.6.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.6.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.6.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.6.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.6.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.6.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.6.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.6.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.6.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.6.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.6.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.6.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.7.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.7.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.7.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.7.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.7.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.7.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.7.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.7.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.7.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.7.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.7.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.7.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.7.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.7.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.7.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.7.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.8.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.8.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.8.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.8.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.8.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.8.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.8.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.8.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.8.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.8.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.8.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.8.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.8.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.8.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.8.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.8.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.9.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.9.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.9.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.9.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.9.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.9.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.9.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.9.attention.output.dense.bias:torch.Size([768])  0%|          | 0/895 [00:00<?, ?it/s, loss:{0:<6.5f}]Epoch 1/5:   0%|          | 0/895 [00:00<?, ?it/s, loss:{0:<6.5f}]Epoch 1/5:   1%|          | 5/895 [00:01<03:58,  3.73it/s, loss:{0:<6.5f}]Epoch 1/5:   1%|          | 5/895 [00:01<03:58,  3.73it/s, loss:389.72625]Epoch 1/5:   1%|          | 10/895 [00:02<03:38,  4.06it/s, loss:389.72625]Epoch 1/5:   1%|          | 10/895 [00:02<03:38,  4.06it/s, loss:282.75001]Epoch 1/5:   2%|▏         | 15/895 [00:03<03:21,  4.37it/s, loss:282.75001]Epoch 1/5:   2%|▏         | 15/895 [00:03<03:21,  4.37it/s, loss:126.45673]Epoch 1/5:   2%|▏         | 20/895 [00:04<03:10,  4.59it/s, loss:126.45673]Epoch 1/5:   2%|▏         | 20/895 [00:04<03:10,  4.59it/s, loss:45.73964] Epoch 1/5:   3%|▎         | 25/895 [00:05<03:01,  4.79it/s, loss:45.73964]Epoch 1/5:   3%|▎         | 25/895 [00:05<03:01,  4.79it/s, loss:44.52793]Epoch 1/5:   3%|▎         | 30/895 [00:06<02:56,  4.89it/s, loss:44.52793]Epoch 1/5:   3%|▎         | 30/895 [00:06<02:56,  4.89it/s, loss:37.94225]Epoch 1/5:   4%|▍         | 35/895 [00:07<02:51,  5.01it/s, loss:37.94225]Epoch 1/5:   4%|▍         | 35/895 [00:07<02:51,  5.01it/s, loss:38.72815]Epoch 1/5:   4%|▍         | 40/895 [00:08<02:48,  5.07it/s, loss:38.72815]Epoch 1/5:   4%|▍         | 40/895 [00:08<02:48,  5.07it/s, loss:24.19028]Epoch 1/5:   5%|▌         | 45/895 [00:08<02:45,  5.14it/s, loss:24.19028]Epoch 1/5:   5%|▌         | 45/895 [00:08<02:45,  5.14it/s, loss:15.46404]Epoch 1/5:   6%|▌         | 50/895 [00:09<02:43,  5.17it/s, loss:15.46404]Epoch 1/5:   6%|▌         | 50/895 [00:09<02:43,  5.17it/s, loss:19.91105]Epoch 1/5:   6%|▌         | 55/895 [00:10<02:42,  5.18it/s, loss:19.91105]Epoch 1/5:   6%|▌         | 55/895 [00:10<02:42,  5.18it/s, loss:14.60597]Epoch 1/5:   7%|▋         | 60/895 [00:11<02:40,  5.20it/s, loss:14.60597]Epoch 1/5:   7%|▋         | 60/895 [00:11<02:40,  5.20it/s, loss:13.95683]Epoch 1/5:   7%|▋         | 65/895 [00:12<02:39,  5.22it/s, loss:13.95683]Epoch 1/5:   7%|▋         | 65/895 [00:12<02:39,  5.22it/s, loss:11.72159]Epoch 1/5:   8%|▊         | 70/895 [00:13<02:37,  5.23it/s, loss:11.72159]Epoch 1/5:   8%|▊         | 70/895 [00:13<02:37,  5.23it/s, loss:9.95649] Epoch 1/5:   8%|▊         | 75/895 [00:14<02:37,  5.22it/s, loss:9.95649]Epoch 1/5:   8%|▊         | 75/895 [00:14<02:37,  5.22it/s, loss:9.53508]Epoch 1/5:   9%|▉         | 80/895 [00:15<02:35,  5.23it/s, loss:9.53508]Epoch 1/5:   9%|▉         | 80/895 [00:15<02:35,  5.23it/s, loss:9.24153]Epoch 1/5:   9%|▉         | 85/895 [00:16<02:34,  5.24it/s, loss:9.24153]Epoch 1/5:   9%|▉         | 85/895 [00:16<02:34,  5.24it/s, loss:6.47042]Epoch 1/5:  10%|█         | 90/895 [00:17<02:33,  5.25it/s, loss:6.47042]Epoch 1/5:  10%|█         | 90/895 [00:17<02:33,  5.25it/s, loss:8.58734]Epoch 1/5:  11%|█         | 95/895 [00:18<02:32,  5.25it/s, loss:8.58734]Epoch 1/5:  11%|█         | 95/895 [00:18<02:32,  5.25it/s, loss:7.64728]Epoch 1/5:  11%|█         | 100/895 [00:19<02:31,  5.26it/s, loss:7.64728]Epoch 1/5:  11%|█         | 100/895 [00:19<02:31,  5.26it/s, loss:5.53915]Epoch 1/5:  12%|█▏        | 105/895 [00:20<02:30,  5.25it/s, loss:5.53915]Epoch 1/5:  12%|█▏        | 105/895 [00:20<02:30,  5.25it/s, loss:3.54893]Epoch 1/5:  12%|█▏        | 110/895 [00:21<02:29,  5.27it/s, loss:3.54893]Epoch 1/5:  12%|█▏        | 110/895 [00:21<02:29,  5.27it/s, loss:3.53622]Epoch 1/5:  13%|█▎        | 115/895 [00:22<02:28,  5.26it/s, loss:3.53622]Epoch 1/5:  13%|█▎        | 115/895 [00:22<02:28,  5.26it/s, loss:3.88632]Epoch 1/5:  13%|█▎        | 120/895 [00:23<02:27,  5.27it/s, loss:3.88632]Epoch 1/5:  13%|█▎        | 120/895 [00:23<02:27,  5.27it/s, loss:4.25069]Epoch 1/5:  14%|█▍        | 125/895 [00:24<02:26,  5.26it/s, loss:4.25069]Epoch 1/5:  14%|█▍        | 125/895 [00:24<02:26,  5.26it/s, loss:4.53912]Epoch 1/5:  15%|█▍        | 130/895 [00:25<02:25,  5.27it/s, loss:4.53912]Epoch 1/5:  15%|█▍        | 130/895 [00:25<02:25,  5.27it/s, loss:4.58343]Epoch 1/5:  15%|█▌        | 135/895 [00:26<02:24,  5.25it/s, loss:4.58343]Epoch 1/5:  15%|█▌        | 135/895 [00:26<02:24,  5.25it/s, loss:3.65378]Epoch 1/5:  16%|█▌        | 140/895 [00:27<02:23,  5.26it/s, loss:3.65378]Epoch 1/5:  16%|█▌        | 140/895 [00:27<02:23,  5.26it/s, loss:3.92905]Epoch 1/5:  16%|█▌        | 145/895 [00:28<02:22,  5.25it/s, loss:3.92905]Epoch 1/5:  16%|█▌        | 145/895 [00:28<02:22,  5.25it/s, loss:3.22202]Epoch 1/5:  17%|█▋        | 150/895 [00:28<02:21,  5.26it/s, loss:3.22202]Epoch 1/5:  17%|█▋        | 150/895 [00:28<02:21,  5.26it/s, loss:2.28050]Epoch 1/5:  17%|█▋        | 155/895 [00:29<02:20,  5.26it/s, loss:2.28050]Epoch 1/5:  17%|█▋        | 155/895 [00:29<02:20,  5.26it/s, loss:5.29193]Epoch 1/5:  18%|█▊        | 160/895 [00:30<02:19,  5.26it/s, loss:5.29193]Epoch 1/5:  18%|█▊        | 160/895 [00:30<02:19,  5.26it/s, loss:5.06146]Epoch 1/5:  18%|█▊        | 165/895 [00:31<02:18,  5.25it/s, loss:5.06146]Epoch 1/5:  18%|█▊        | 165/895 [00:31<02:18,  5.25it/s, loss:4.46475]Epoch 1/5:  19%|█▉        | 170/895 [00:32<02:18,  5.25it/s, loss:4.46475]Epoch 1/5:  19%|█▉        | 170/895 [00:32<02:18,  5.25it/s, loss:3.08226]Epoch 1/5:  20%|█▉        | 175/895 [00:33<02:16,  5.26it/s, loss:3.08226]Epoch 1/5:  20%|█▉        | 175/895 [00:33<02:16,  5.26it/s, loss:2.85750]                                                                          Epoch 1/5:  20%|█▉        | 175/895 [00:36<02:16,  5.26it/s, loss:2.85750]                                                                          Epoch 1/5:  20%|█▉        | 175/895 [00:39<02:16,  5.26it/s, loss:2.85750]                                                                          Epoch 1/5:  20%|█▉        | 175/895 [00:39<02:16,  5.26it/s, loss:2.85750]/home/dy/anaconda3/envs/flat/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 2/5:  20%|█▉        | 175/895 [00:39<02:16,  5.26it/s, loss:2.85750]Epoch 2/5:  20%|██        | 180/895 [00:39<05:45,  2.07it/s, loss:2.85750]Epoch 2/5:  20%|██        | 180/895 [00:39<05:45,  2.07it/s, loss:3.56082]Epoch 2/5:  21%|██        | 185/895 [00:40<04:41,  2.52it/s, loss:3.56082]Epoch 2/5:  21%|██        | 185/895 [00:40<04:41,  2.52it/s, loss:2.87472]Epoch 2/5:  21%|██        | 190/895 [00:41<03:55,  2.99it/s, loss:2.87472]Epoch 2/5:  21%|██        | 190/895 [00:41<03:55,  2.99it/s, loss:1.62502]Epoch 2/5:  22%|██▏       | 195/895 [00:42<03:23,  3.43it/s, loss:1.62502]Epoch 2/5:  22%|██▏       | 195/895 [00:42<03:23,  3.43it/s, loss:4.89123]Epoch 2/5:  22%|██▏       | 200/895 [00:43<03:01,  3.84it/s, loss:4.89123]Epoch 2/5:  22%|██▏       | 200/895 [00:43<03:01,  3.84it/s, loss:2.58458]Epoch 2/5:  23%|██▎       | 205/895 [00:44<02:45,  4.17it/s, loss:2.58458]Epoch 2/5:  23%|██▎       | 205/895 [00:44<02:45,  4.17it/s, loss:2.92743]Epoch 2/5:  23%|██▎       | 210/895 [00:45<02:33,  4.45it/s, loss:2.92743]Epoch 2/5:  23%|██▎       | 210/895 [00:45<02:33,  4.45it/s, loss:2.02252]Epoch 2/5:  24%|██▍       | 215/895 [00:46<02:26,  4.65it/s, loss:2.02252]Epoch 2/5:  24%|██▍       | 215/895 [00:46<02:26,  4.65it/s, loss:2.86992]Epoch 2/5:  25%|██▍       | 220/895 [00:47<02:19,  4.83it/s, loss:2.86992]Epoch 2/5:  25%|██▍       | 220/895 [00:47<02:19,  4.83it/s, loss:2.58787]Epoch 2/5:  25%|██▌       | 225/895 [00:48<02:15,  4.94it/s, loss:2.58787]Epoch 2/5:  25%|██▌       | 225/895 [00:48<02:15,  4.94it/s, loss:2.56141]Epoch 2/5:  26%|██▌       | 230/895 [00:49<02:12,  5.01it/s, loss:2.56141]Epoch 2/5:  26%|██▌       | 230/895 [00:49<02:12,  5.01it/s, loss:2.61865]Epoch 2/5:  26%|██▋       | 235/895 [00:50<02:10,  5.06it/s, loss:2.61865]Epoch 2/5:  26%|██▋       | 235/895 [00:50<02:10,  5.06it/s, loss:2.51580]Epoch 2/5:  27%|██▋       | 240/895 [00:51<02:07,  5.13it/s, loss:2.51580]Epoch 2/5:  27%|██▋       | 240/895 [00:51<02:07,  5.13it/s, loss:2.40201]Epoch 2/5:  27%|██▋       | 245/895 [00:51<02:06,  5.15it/s, loss:2.40201]Epoch 2/5:  27%|██▋       | 245/895 [00:51<02:06,  5.15it/s, loss:2.68361]Epoch 2/5:  28%|██▊       | 250/895 [00:52<02:04,  5.19it/s, loss:2.68361]Epoch 2/5:  28%|██▊       | 250/895 [00:52<02:04,  5.19it/s, loss:3.10964]Epoch 2/5:  28%|██▊       | 255/895 [00:53<02:02,  5.21it/s, loss:3.10964]Epoch 2/5:  28%|██▊       | 255/895 [00:53<02:02,  5.21it/s, loss:3.06898]Epoch 2/5:  29%|██▉       | 260/895 [00:54<02:01,  5.22it/s, loss:3.06898]Epoch 2/5:  29%|██▉       | 260/895 [00:54<02:01,  5.22it/s, loss:2.78704]Epoch 2/5:  30%|██▉       | 265/895 [00:55<02:00,  5.23it/s, loss:2.78704]Epoch 2/5:  30%|██▉       | 265/895 [00:55<02:00,  5.23it/s, loss:2.70149]Epoch 2/5:  30%|███       | 270/895 [00:56<01:59,  5.23it/s, loss:2.70149]Epoch 2/5:  30%|███       | 270/895 [00:56<01:59,  5.23it/s, loss:2.39329]Epoch 2/5:  31%|███       | 275/895 [00:57<01:58,  5.24it/s, loss:2.39329]Epoch 2/5:  31%|███       | 275/895 [00:57<01:58,  5.24it/s, loss:2.68134]Epoch 2/5:  31%|███▏      | 280/895 [00:58<01:57,  5.23it/s, loss:2.68134]Epoch 2/5:  31%|███▏      | 280/895 [00:58<01:57,  5.23it/s, loss:2.29391]Epoch 2/5:  32%|███▏      | 285/895 [00:59<01:56,  5.24it/s, loss:2.29391]Epoch 2/5:  32%|███▏      | 285/895 [00:59<01:56,  5.24it/s, loss:2.70286]Epoch 2/5:  32%|███▏      | 290/895 [01:00<01:55,  5.24it/s, loss:2.70286]Epoch 2/5:  32%|███▏      | 290/895 [01:00<01:55,  5.24it/s, loss:1.96793]Epoch 2/5:  33%|███▎      | 295/895 [01:01<01:54,  5.25it/s, loss:1.96793]Epoch 2/5:  33%|███▎      | 295/895 [01:01<01:54,  5.25it/s, loss:3.16037]Epoch 2/5:  34%|███▎      | 300/895 [01:02<01:53,  5.23it/s, loss:3.16037]Epoch 2/5:  34%|███▎      | 300/895 [01:02<01:53,  5.23it/s, loss:2.15122]Epoch 2/5:  34%|███▍      | 305/895 [01:03<01:52,  5.25it/s, loss:2.15122]Epoch 2/5:  34%|███▍      | 305/895 [01:03<01:52,  5.25it/s, loss:2.13387]Epoch 2/5:  35%|███▍      | 310/895 [01:04<01:51,  5.23it/s, loss:2.13387]Epoch 2/5:  35%|███▍      | 310/895 [01:04<01:51,  5.23it/s, loss:2.92548]Epoch 2/5:  35%|███▌      | 315/895 [01:05<01:50,  5.25it/s, loss:2.92548]Epoch 2/5:  35%|███▌      | 315/895 [01:05<01:50,  5.25it/s, loss:2.47462]Epoch 2/5:  36%|███▌      | 320/895 [01:06<01:49,  5.23it/s, loss:2.47462]Epoch 2/5:  36%|███▌      | 320/895 [01:06<01:49,  5.23it/s, loss:2.13403]Epoch 2/5:  36%|███▋      | 325/895 [01:07<01:48,  5.25it/s, loss:2.13403]Epoch 2/5:  36%|███▋      | 325/895 [01:07<01:48,  5.25it/s, loss:1.86709]Epoch 2/5:  37%|███▋      | 330/895 [01:08<01:47,  5.24it/s, loss:1.86709]Epoch 2/5:  37%|███▋      | 330/895 [01:08<01:47,  5.24it/s, loss:2.55848]Epoch 2/5:  37%|███▋      | 335/895 [01:09<01:46,  5.25it/s, loss:2.55848]Epoch 2/5:  37%|███▋      | 335/895 [01:09<01:46,  5.25it/s, loss:1.95595]Epoch 2/5:  38%|███▊      | 340/895 [01:10<01:46,  5.23it/s, loss:1.95595]Epoch 2/5:  38%|███▊      | 340/895 [01:10<01:46,  5.23it/s, loss:2.31738]Epoch 2/5:  39%|███▊      | 345/895 [01:11<01:44,  5.25it/s, loss:2.31738]Epoch 2/5:  39%|███▊      | 345/895 [01:11<01:44,  5.25it/s, loss:1.95333]Epoch 2/5:  39%|███▉      | 350/895 [01:12<01:46,  5.14it/s, loss:1.95333]Epoch 2/5:  39%|███▉      | 350/895 [01:12<01:46,  5.14it/s, loss:2.41748]Epoch 2/5:  40%|███▉      | 355/895 [01:12<01:44,  5.18it/s, loss:2.41748]Epoch 2/5:  40%|███▉      | 355/895 [01:12<01:44,  5.18it/s, loss:1.71929]                                                                          Epoch 2/5:  40%|███▉      | 355/895 [01:15<01:44,  5.18it/s, loss:1.71929]                                                                          Epoch 2/5:  40%|███▉      | 355/895 [01:18<01:44,  5.18it/s, loss:1.71929]                                                                          Epoch 2/5:  40%|███▉      | 355/895 [01:18<01:44,  5.18it/s, loss:1.71929]Epoch 3/5:  40%|███▉      | 355/895 [01:18<01:44,  5.18it/s, loss:1.71929]Epoch 3/5:  40%|████      | 360/895 [01:18<04:15,  2.10it/s, loss:1.71929]Epoch 3/5:  40%|████      | 360/895 [01:18<04:15,  2.10it/s, loss:1.83034]Epoch 3/5:  41%|████      | 365/895 [01:19<03:27,  2.56it/s, loss:1.83034]Epoch 3/5:  41%|████      | 365/895 [01:19<03:27,  2.56it/s, loss:1.50676]Epoch 3/5:  41%|████▏     | 370/895 [01:20<02:53,  3.02it/s, loss:1.50676]Epoch 3/5:  41%|████▏     | 370/895 [01:20<02:53,  3.02it/s, loss:1.22751]Epoch 3/5:  42%|████▏     | 375/895 [01:21<02:30,  3.46it/s, loss:1.22751]Epoch 3/5:  42%|████▏     | 375/895 [01:21<02:30,  3.46it/s, loss:1.83416]Epoch 3/5:  42%|████▏     | 380/895 [01:22<02:13,  3.85it/s, loss:1.83416]Epoch 3/5:  42%|████▏     | 380/895 [01:22<02:13,  3.85it/s, loss:1.06456]Epoch 3/5:  43%|████▎     | 385/895 [01:23<02:01,  4.19it/s, loss:1.06456]Epoch 3/5:  43%|████▎     | 385/895 [01:23<02:01,  4.19it/s, loss:2.10035]Epoch 3/5:  44%|████▎     | 390/895 [01:24<01:53,  4.45it/s, loss:2.10035]Epoch 3/5:  44%|████▎     | 390/895 [01:24<01:53,  4.45it/s, loss:1.19911]Epoch 3/5:  44%|████▍     | 395/895 [01:25<01:47,  4.67it/s, loss:1.19911]Epoch 3/5:  44%|████▍     | 395/895 [01:25<01:47,  4.67it/s, loss:1.37911]Epoch 3/5:  45%|████▍     | 400/895 [01:26<01:42,  4.82it/s, loss:1.37911]Epoch 3/5:  45%|████▍     | 400/895 [01:26<01:42,  4.82it/s, loss:1.54349]Epoch 3/5:  45%|████▌     | 405/895 [01:27<01:39,  4.95it/s, loss:1.54349]Epoch 3/5:  45%|████▌     | 405/895 [01:27<01:39,  4.95it/s, loss:1.39824]Epoch 3/5:  46%|████▌     | 410/895 [01:28<01:37,  4.95it/s, loss:1.39824]Epoch 3/5:  46%|████▌     | 410/895 [01:28<01:37,  4.95it/s, loss:1.84335]Epoch 3/5:  46%|████▋     | 415/895 [01:29<01:35,  5.04it/s, loss:1.84335]Epoch 3/5:  46%|████▋     | 415/895 [01:29<01:35,  5.04it/s, loss:1.45708]Epoch 3/5:  47%|████▋     | 420/895 [01:30<01:33,  5.09it/s, loss:1.45708]Epoch 3/5:  47%|████▋     | 420/895 [01:30<01:33,  5.09it/s, loss:1.97971]Epoch 3/5:  47%|████▋     | 425/895 [01:31<01:31,  5.14it/s, loss:1.97971]Epoch 3/5:  47%|████▋     | 425/895 [01:31<01:31,  5.14it/s, loss:1.60374]Epoch 3/5:  48%|████▊     | 430/895 [01:32<01:30,  5.16it/s, loss:1.60374]Epoch 3/5:  48%|████▊     | 430/895 [01:32<01:30,  5.16it/s, loss:1.58796]Epoch 3/5:  49%|████▊     | 435/895 [01:33<01:28,  5.20it/s, loss:1.58796]Epoch 3/5:  49%|████▊     | 435/895 [01:33<01:28,  5.20it/s, loss:1.54747]Epoch 3/5:  49%|████▉     | 440/895 [01:34<01:29,  5.10it/s, loss:1.54747]Epoch 3/5:  49%|████▉     | 440/895 [01:34<01:29,  5.10it/s, loss:2.01641]Epoch 3/5:  50%|████▉     | 445/895 [01:35<01:27,  5.15it/s, loss:2.01641]Epoch 3/5:  50%|████▉     | 445/895 [01:35<01:27,  5.15it/s, loss:1.75524]Epoch 3/5:  50%|█████     | 450/895 [01:35<01:26,  5.17it/s, loss:1.75524]Epoch 3/5:  50%|█████     | 450/895 [01:35<01:26,  5.17it/s, loss:1.40472]Epoch 3/5:  51%|█████     | 455/895 [01:36<01:24,  5.19it/s, loss:1.40472]Epoch 3/5:  51%|█████     | 455/895 [01:36<01:24,  5.19it/s, loss:1.51213]Epoch 3/5:  51%|█████▏    | 460/895 [01:37<01:23,  5.21it/s, loss:1.51213]Epoch 3/5:  51%|█████▏    | 460/895 [01:37<01:23,  5.21it/s, loss:1.59619]Epoch 3/5:  52%|█████▏    | 465/895 [01:38<01:22,  5.21it/s, loss:1.59619]Epoch 3/5:  52%|█████▏    | 465/895 [01:38<01:22,  5.21it/s, loss:1.74545]Epoch 3/5:  53%|█████▎    | 470/895 [01:39<01:21,  5.22it/s, loss:1.74545]Epoch 3/5:  53%|█████▎    | 470/895 [01:39<01:21,  5.22it/s, loss:1.44022]Epoch 3/5:  53%|█████▎    | 475/895 [01:40<01:20,  5.22it/s, loss:1.44022]Epoch 3/5:  53%|█████▎    | 475/895 [01:40<01:20,  5.22it/s, loss:1.13310]Epoch 3/5:  54%|█████▎    | 480/895 [01:41<01:19,  5.23it/s, loss:1.13310]Epoch 3/5:  54%|█████▎    | 480/895 [01:41<01:19,  5.23it/s, loss:1.11719]Epoch 3/5:  54%|█████▍    | 485/895 [01:42<01:18,  5.23it/s, loss:1.11719]Epoch 3/5:  54%|█████▍    | 485/895 [01:42<01:18,  5.23it/s, loss:1.42155]Epoch 3/5:  55%|█████▍    | 490/895 [01:43<01:17,  5.23it/s, loss:1.42155]Epoch 3/5:  55%|█████▍    | 490/895 [01:43<01:17,  5.23it/s, loss:1.55605]Epoch 3/5:  55%|█████▌    | 495/895 [01:44<01:16,  5.23it/s, loss:1.55605]Epoch 3/5:  55%|█████▌    | 495/895 [01:44<01:16,  5.23it/s, loss:1.04729]Epoch 3/5:  56%|█████▌    | 500/895 [01:45<01:15,  5.24it/s, loss:1.04729]Epoch 3/5:  56%|█████▌    | 500/895 [01:45<01:15,  5.24it/s, loss:1.50441]Epoch 3/5:  56%|█████▋    | 505/895 [01:46<01:14,  5.22it/s, loss:1.50441]Epoch 3/5:  56%|█████▋    | 505/895 [01:46<01:14,  5.22it/s, loss:1.15306]Epoch 3/5:  57%|█████▋    | 510/895 [01:47<01:13,  5.24it/s, loss:1.15306]Epoch 3/5:  57%|█████▋    | 510/895 [01:47<01:13,  5.24it/s, loss:1.72184]Epoch 3/5:  58%|█████▊    | 515/895 [01:48<01:12,  5.23it/s, loss:1.72184]Epoch 3/5:  58%|█████▊    | 515/895 [01:48<01:12,  5.23it/s, loss:1.39958]Epoch 3/5:  58%|█████▊    | 520/895 [01:49<01:11,  5.24it/s, loss:1.39958]Epoch 3/5:  58%|█████▊    | 520/895 [01:49<01:11,  5.24it/s, loss:1.76114]Epoch 3/5:  59%|█████▊    | 525/895 [01:50<01:10,  5.22it/s, loss:1.76114]Epoch 3/5:  59%|█████▊    | 525/895 [01:50<01:10,  5.22it/s, loss:1.84776]Epoch 3/5:  59%|█████▉    | 530/895 [01:51<01:09,  5.24it/s, loss:1.84776]Epoch 3/5:  59%|█████▉    | 530/895 [01:51<01:09,  5.24it/s, loss:2.10941]Epoch 3/5:  60%|█████▉    | 535/895 [01:52<01:08,  5.23it/s, loss:2.10941]Epoch 3/5:  60%|█████▉    | 535/895 [01:52<01:08,  5.23it/s, loss:1.79266]                                                                          Epoch 3/5:  60%|█████▉    | 535/895 [01:54<01:08,  5.23it/s, loss:1.79266]                                                                          Epoch 3/5:  60%|█████▉    | 535/895 [01:57<01:08,  5.23it/s, loss:1.79266]                                                                          Epoch 3/5:  60%|█████▉    | 535/895 [01:57<01:08,  5.23it/s, loss:1.79266]Epoch 4/5:  60%|█████▉    | 535/895 [01:57<01:08,  5.23it/s, loss:1.79266]Epoch 4/5:  60%|██████    | 540/895 [01:57<02:48,  2.11it/s, loss:1.79266]Epoch 4/5:  60%|██████    | 540/895 [01:57<02:48,  2.11it/s, loss:1.24632]Epoch 4/5:  61%|██████    | 545/895 [01:58<02:16,  2.57it/s, loss:1.24632]Epoch 4/5:  61%|██████    | 545/895 [01:58<02:16,  2.57it/s, loss:0.95411]Epoch 4/5:  61%|██████▏   | 550/895 [01:59<01:53,  3.04it/s, loss:0.95411]Epoch 4/5:  61%|██████▏   | 550/895 [01:59<01:53,  3.04it/s, loss:0.94671]Epoch 4/5:  62%|██████▏   | 555/895 [02:00<01:37,  3.48it/s, loss:0.94671]Epoch 4/5:  62%|██████▏   | 555/895 [02:00<01:37,  3.48it/s, loss:1.21408]Epoch 4/5:  63%|██████▎   | 560/895 [02:01<01:26,  3.87it/s, loss:1.21408]Epoch 4/5:  63%|██████▎   | 560/895 [02:01<01:26,  3.87it/s, loss:0.86871]Epoch 4/5:  63%|██████▎   | 565/895 [02:02<01:18,  4.19it/s, loss:0.86871]Epoch 4/5:  63%|██████▎   | 565/895 [02:02<01:18,  4.19it/s, loss:1.71219]Epoch 4/5:  64%|██████▎   | 570/895 [02:03<01:12,  4.47it/s, loss:1.71219]Epoch 4/5:  64%|██████▎   | 570/895 [02:03<01:12,  4.47it/s, loss:1.16734]Epoch 4/5:  64%|██████▍   | 575/895 [02:04<01:08,  4.66it/s, loss:1.16734]Epoch 4/5:  64%|██████▍   | 575/895 [02:04<01:08,  4.66it/s, loss:1.50667]Epoch 4/5:  65%|██████▍   | 580/895 [02:05<01:05,  4.83it/s, loss:1.50667]Epoch 4/5:  65%|██████▍   | 580/895 [02:05<01:05,  4.83it/s, loss:2.09668]Epoch 4/5:  65%|██████▌   | 585/895 [02:06<01:02,  4.93it/s, loss:2.09668]Epoch 4/5:  65%|██████▌   | 585/895 [02:06<01:02,  4.93it/s, loss:1.33645]Epoch 4/5:  66%|██████▌   | 590/895 [02:07<01:00,  5.03it/s, loss:1.33645]Epoch 4/5:  66%|██████▌   | 590/895 [02:07<01:00,  5.03it/s, loss:1.72111]Epoch 4/5:  66%|██████▋   | 595/895 [02:08<00:59,  5.08it/s, loss:1.72111]Epoch 4/5:  66%|██████▋   | 595/895 [02:08<00:59,  5.08it/s, loss:0.88226]Epoch 4/5:  67%|██████▋   | 600/895 [02:09<00:57,  5.14it/s, loss:0.88226]Epoch 4/5:  67%|██████▋   | 600/895 [02:09<00:57,  5.14it/s, loss:1.35313]Epoch 4/5:  68%|██████▊   | 605/895 [02:10<00:56,  5.15it/s, loss:1.35313]Epoch 4/5:  68%|██████▊   | 605/895 [02:10<00:56,  5.15it/s, loss:1.34238]Epoch 4/5:  68%|██████▊   | 610/895 [02:11<00:54,  5.19it/s, loss:1.34238]Epoch 4/5:  68%|██████▊   | 610/895 [02:11<00:54,  5.19it/s, loss:0.84439]Epoch 4/5:  69%|██████▊   | 615/895 [02:12<00:53,  5.19it/s, loss:0.84439]Epoch 4/5:  69%|██████▊   | 615/895 [02:12<00:53,  5.19it/s, loss:1.08161]Epoch 4/5:  69%|██████▉   | 620/895 [02:13<00:52,  5.22it/s, loss:1.08161]Epoch 4/5:  69%|██████▉   | 620/895 [02:13<00:52,  5.22it/s, loss:1.16725]Epoch 4/5:  70%|██████▉   | 625/895 [02:14<00:51,  5.21it/s, loss:1.16725]Epoch 4/5:  70%|██████▉   | 625/895 [02:14<00:51,  5.21it/s, loss:0.73757]Epoch 4/5:  70%|███████   | 630/895 [02:15<00:50,  5.23it/s, loss:0.73757]Epoch 4/5:  70%|███████   | 630/895 [02:15<00:50,  5.23it/s, loss:1.38022]Epoch 4/5:  71%|███████   | 635/895 [02:16<00:49,  5.23it/s, loss:1.38022]Epoch 4/5:  71%|███████   | 635/895 [02:16<00:49,  5.23it/s, loss:1.72842]Epoch 4/5:  72%|███████▏  | 640/895 [02:16<00:48,  5.24it/s, loss:1.72842]Epoch 4/5:  72%|███████▏  | 640/895 [02:16<00:48,  5.24it/s, loss:1.14277]Epoch 4/5:  72%|███████▏  | 645/895 [02:17<00:47,  5.24it/s, loss:1.14277]Epoch 4/5:  72%|███████▏  | 645/895 [02:17<00:47,  5.24it/s, loss:1.39245]Epoch 4/5:  73%|███████▎  | 650/895 [02:18<00:46,  5.24it/s, loss:1.39245]Epoch 4/5:  73%|███████▎  | 650/895 [02:18<00:46,  5.24it/s, loss:0.92588]Epoch 4/5:  73%|███████▎  | 655/895 [02:19<00:45,  5.24it/s, loss:0.92588]Epoch 4/5:  73%|███████▎  | 655/895 [02:19<00:45,  5.24it/s, loss:1.08302]Epoch 4/5:  74%|███████▎  | 660/895 [02:20<00:45,  5.17it/s, loss:1.08302]Epoch 4/5:  74%|███████▎  | 660/895 [02:20<00:45,  5.17it/s, loss:0.84398]Epoch 4/5:  74%|███████▍  | 665/895 [02:21<00:44,  5.19it/s, loss:0.84398]Epoch 4/5:  74%|███████▍  | 665/895 [02:21<00:44,  5.19it/s, loss:1.18798]Epoch 4/5:  75%|███████▍  | 670/895 [02:22<00:43,  5.21it/s, loss:1.18798]Epoch 4/5:  75%|███████▍  | 670/895 [02:22<00:43,  5.21it/s, loss:1.15165]Epoch 4/5:  75%|███████▌  | 675/895 [02:23<00:42,  5.23it/s, loss:1.15165]Epoch 4/5:  75%|███████▌  | 675/895 [02:23<00:42,  5.23it/s, loss:0.71195]Epoch 4/5:  76%|███████▌  | 680/895 [02:24<00:41,  5.24it/s, loss:0.71195]Epoch 4/5:  76%|███████▌  | 680/895 [02:24<00:41,  5.24it/s, loss:1.44135]Epoch 4/5:  77%|███████▋  | 685/895 [02:25<00:39,  5.25it/s, loss:1.44135]Epoch 4/5:  77%|███████▋  | 685/895 [02:25<00:39,  5.25it/s, loss:0.86418]Epoch 4/5:  77%|███████▋  | 690/895 [02:26<00:39,  5.25it/s, loss:0.86418]Epoch 4/5:  77%|███████▋  | 690/895 [02:26<00:39,  5.25it/s, loss:1.27415]Epoch 4/5:  78%|███████▊  | 695/895 [02:27<00:38,  5.26it/s, loss:1.27415]Epoch 4/5:  78%|███████▊  | 695/895 [02:27<00:38,  5.26it/s, loss:1.29518]Epoch 4/5:  78%|███████▊  | 700/895 [02:28<00:37,  5.25it/s, loss:1.29518]Epoch 4/5:  78%|███████▊  | 700/895 [02:28<00:37,  5.25it/s, loss:1.02548]Epoch 4/5:  79%|███████▉  | 705/895 [02:29<00:36,  5.26it/s, loss:1.02548]Epoch 4/5:  79%|███████▉  | 705/895 [02:29<00:36,  5.26it/s, loss:1.32729]Epoch 4/5:  79%|███████▉  | 710/895 [02:30<00:35,  5.25it/s, loss:1.32729]Epoch 4/5:  79%|███████▉  | 710/895 [02:30<00:35,  5.25it/s, loss:1.46534]Epoch 4/5:  80%|███████▉  | 715/895 [02:31<00:34,  5.22it/s, loss:1.46534]Epoch 4/5:  80%|███████▉  | 715/895 [02:31<00:34,  5.22it/s, loss:1.30266]                                                                          Epoch 4/5:  80%|███████▉  | 715/895 [02:33<00:34,  5.22it/s, loss:1.30266]                                                                          Epoch 4/5:  80%|███████▉  | 715/895 [02:33<00:34,  5.22it/s, loss:1.30266]                                                                          Epoch 4/5:  80%|███████▉  | 715/895 [02:33<00:34,  5.22it/s, loss:1.30266]Epoch 5/5:  80%|███████▉  | 715/895 [02:33<00:34,  5.22it/s, loss:1.30266]Epoch 5/5:  80%|████████  | 720/895 [02:34<00:58,  3.01it/s, loss:1.30266]Epoch 5/5:  80%|████████  | 720/895 [02:34<00:58,  3.01it/s, loss:1.35151]Epoch 5/5:  81%|████████  | 725/895 [02:35<00:49,  3.46it/s, loss:1.35151]Epoch 5/5:  81%|████████  | 725/895 [02:35<00:49,  3.46it/s, loss:0.68074]Epoch 5/5:  82%|████████▏ | 730/895 [02:36<00:42,  3.85it/s, loss:0.68074]Epoch 5/5:  82%|████████▏ | 730/895 [02:36<00:42,  3.85it/s, loss:0.69459]Epoch 5/5:  82%|████████▏ | 735/895 [02:37<00:38,  4.19it/s, loss:0.69459]Epoch 5/5:  82%|████████▏ | 735/895 [02:37<00:38,  4.19it/s, loss:0.93695]Epoch 5/5:  83%|████████▎ | 740/895 [02:38<00:34,  4.45it/s, loss:0.93695]Epoch 5/5:  83%|████████▎ | 740/895 [02:38<00:34,  4.45it/s, loss:0.71710]Epoch 5/5:  83%|████████▎ | 745/895 [02:39<00:32,  4.68it/s, loss:0.71710]Epoch 5/5:  83%|████████▎ | 745/895 [02:39<00:32,  4.68it/s, loss:1.10278]Epoch 5/5:  84%|████████▍ | 750/895 [02:40<00:30,  4.83it/s, loss:1.10278]Epoch 5/5:  84%|████████▍ | 750/895 [02:40<00:30,  4.83it/s, loss:1.08739]Epoch 5/5:  84%|████████▍ | 755/895 [02:41<00:28,  4.96it/s, loss:1.08739]Epoch 5/5:  84%|████████▍ | 755/895 [02:41<00:28,  4.96it/s, loss:0.87979]Epoch 5/5:  85%|████████▍ | 760/895 [02:42<00:27,  4.95it/s, loss:0.87979]Epoch 5/5:  85%|████████▍ | 760/895 [02:42<00:27,  4.95it/s, loss:0.74469]Epoch 5/5:  85%|████████▌ | 765/895 [02:43<00:25,  5.05it/s, loss:0.74469]Epoch 5/5:  85%|████████▌ | 765/895 [02:43<00:25,  5.05it/s, loss:0.96680]Epoch 5/5:  86%|████████▌ | 770/895 [02:44<00:24,  5.09it/s, loss:0.96680]Epoch 5/5:  86%|████████▌ | 770/895 [02:44<00:24,  5.09it/s, loss:0.90533]Epoch 5/5:  87%|████████▋ | 775/895 [02:45<00:23,  5.15it/s, loss:0.90533]Epoch 5/5:  87%|████████▋ | 775/895 [02:45<00:23,  5.15it/s, loss:1.14274]Epoch 5/5:  87%|████████▋ | 780/895 [02:46<00:22,  5.17it/s, loss:1.14274]Epoch 5/5:  87%|████████▋ | 780/895 [02:46<00:22,  5.17it/s, loss:0.84403]Epoch 5/5:  88%|████████▊ | 785/895 [02:47<00:21,  5.21it/s, loss:0.84403]Epoch 5/5:  88%|████████▊ | 785/895 [02:47<00:21,  5.21it/s, loss:1.02447]Epoch 5/5:  88%|████████▊ | 790/895 [02:47<00:20,  5.21it/s, loss:1.02447]Epoch 5/5:  88%|████████▊ | 790/895 [02:47<00:20,  5.21it/s, loss:1.09311]Epoch 5/5:  89%|████████▉ | 795/895 [02:48<00:19,  5.23it/s, loss:1.09311]Epoch 5/5:  89%|████████▉ | 795/895 [02:48<00:19,  5.23it/s, loss:1.09371]Epoch 5/5:  89%|████████▉ | 800/895 [02:49<00:18,  5.23it/s, loss:1.09371]Epoch 5/5:  89%|████████▉ | 800/895 [02:49<00:18,  5.23it/s, loss:0.70342]Epoch 5/5:  90%|████████▉ | 805/895 [02:50<00:17,  5.24it/s, loss:0.70342]Epoch 5/5:  90%|████████▉ | 805/895 [02:50<00:17,  5.24it/s, loss:1.08319]Epoch 5/5:  91%|█████████ | 810/895 [02:51<00:16,  5.25it/s, loss:1.08319]Epoch 5/5:  91%|█████████ | 810/895 [02:51<00:16,  5.25it/s, loss:1.14853]Epoch 5/5:  91%|█████████ | 815/895 [02:52<00:15,  5.24it/s, loss:1.14853]Epoch 5/5:  91%|█████████ | 815/895 [02:52<00:15,  5.24it/s, loss:0.83582]Epoch 5/5:  92%|█████████▏| 820/895 [02:53<00:14,  5.25it/s, loss:0.83582]Epoch 5/5:  92%|█████████▏| 820/895 [02:53<00:14,  5.25it/s, loss:0.57614]Epoch 5/5:  92%|█████████▏| 825/895 [02:54<00:13,  5.24it/s, loss:0.57614]Epoch 5/5:  92%|█████████▏| 825/895 [02:54<00:13,  5.24it/s, loss:0.82312]Epoch 5/5:  93%|█████████▎| 830/895 [02:55<00:12,  5.25it/s, loss:0.82312]Epoch 5/5:  93%|█████████▎| 830/895 [02:55<00:12,  5.25it/s, loss:0.80758]Epoch 5/5:  93%|█████████▎| 835/895 [02:56<00:11,  5.24it/s, loss:0.80758]Epoch 5/5:  93%|█████████▎| 835/895 [02:56<00:11,  5.24it/s, loss:0.78061]Epoch 5/5:  94%|█████████▍| 840/895 [02:57<00:10,  5.25it/s, loss:0.78061]Epoch 5/5:  94%|█████████▍| 840/895 [02:57<00:10,  5.25it/s, loss:0.49455]Epoch 5/5:  94%|█████████▍| 845/895 [02:58<00:09,  5.24it/s, loss:0.49455]Epoch 5/5:  94%|█████████▍| 845/895 [02:58<00:09,  5.24it/s, loss:1.07340]Epoch 5/5:  95%|█████████▍| 850/895 [02:59<00:08,  5.25it/s, loss:1.07340]Epoch 5/5:  95%|█████████▍| 850/895 [02:59<00:08,  5.25it/s, loss:0.88956]Epoch 5/5:  96%|█████████▌| 855/895 [03:00<00:07,  5.24it/s, loss:0.88956]Epoch 5/5:  96%|█████████▌| 855/895 [03:00<00:07,  5.24it/s, loss:1.06426]Epoch 5/5:  96%|█████████▌| 860/895 [03:01<00:06,  5.26it/s, loss:1.06426]Epoch 5/5:  96%|█████████▌| 860/895 [03:01<00:06,  5.26it/s, loss:0.76839]Epoch 5/5:  97%|█████████▋| 865/895 [03:02<00:05,  5.24it/s, loss:0.76839]Epoch 5/5:  97%|█████████▋| 865/895 [03:02<00:05,  5.24it/s, loss:0.88271]Epoch 5/5:  97%|█████████▋| 870/895 [03:03<00:04,  5.25it/s, loss:0.88271]Epoch 5/5:  97%|█████████▋| 870/895 [03:03<00:04,  5.25it/s, loss:0.80262]Epoch 5/5:  98%|█████████▊| 875/895 [03:04<00:03,  5.24it/s, loss:0.80262]Epoch 5/5:  98%|█████████▊| 875/895 [03:04<00:03,  5.24it/s, loss:0.77736]Epoch 5/5:  98%|█████████▊| 880/895 [03:05<00:02,  5.25it/s, loss:0.77736]Epoch 5/5:  98%|█████████▊| 880/895 [03:05<00:02,  5.25it/s, loss:0.93175]Epoch 5/5:  99%|█████████▉| 885/895 [03:06<00:01,  5.23it/s, loss:0.93175]Epoch 5/5:  99%|█████████▉| 885/895 [03:06<00:01,  5.23it/s, loss:0.69079]Epoch 5/5:  99%|█████████▉| 890/895 [03:07<00:00,  5.25it/s, loss:0.69079]Epoch 5/5:  99%|█████████▉| 890/895 [03:07<00:00,  5.25it/s, loss:0.70245]Epoch 5/5: 100%|██████████| 895/895 [03:07<00:00,  5.33it/s, loss:0.70245]Epoch 5/5: 100%|██████████| 895/895 [03:07<00:00,  5.33it/s, loss:0.89549]                                                                          Epoch 5/5: 100%|██████████| 895/895 [03:10<00:00,  5.33it/s, loss:0.89549]                                                                          Epoch 5/5: 100%|██████████| 895/895 [03:12<00:00,  5.33it/s, loss:0.89549]                                                                          Epoch 5/5: 100%|██████████| 895/895 [03:12<00:00,  5.33it/s, loss:0.89549]                                                                          
bert_embedding.model.encoder.encoder.layer.9.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.9.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.9.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.9.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.9.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.9.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.9.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.9.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.10.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.10.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.10.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.10.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.10.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.10.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.10.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.10.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.10.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.10.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.10.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.10.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.10.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.10.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.10.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.10.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.11.attention.self.query.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.11.attention.self.query.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.11.attention.self.key.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.11.attention.self.key.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.11.attention.self.value.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.11.attention.self.value.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.11.attention.output.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.encoder.layer.11.attention.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.11.attention.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.11.attention.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.11.intermediate.dense.weight:torch.Size([3072, 768])
bert_embedding.model.encoder.encoder.layer.11.intermediate.dense.bias:torch.Size([3072])
bert_embedding.model.encoder.encoder.layer.11.output.dense.weight:torch.Size([768, 3072])
bert_embedding.model.encoder.encoder.layer.11.output.dense.bias:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.11.output.LayerNorm.weight:torch.Size([768])
bert_embedding.model.encoder.encoder.layer.11.output.LayerNorm.bias:torch.Size([768])
bert_embedding.model.encoder.pooler.dense.weight:torch.Size([768, 768])
bert_embedding.model.encoder.pooler.dense.bias:torch.Size([768])
pos_embedding.weight:torch.Size([55, 20])
output.weight:torch.Size([21, 1556])
output.bias:torch.Size([21])
crf.start_transitions:torch.Size([21])
crf.end_transitions:torch.Size([21])
crf.transitions:torch.Size([21, 21])
***************init pram***************
xavier uniform init:output.weight
***************init pram***************
layernum: 1
label num: 21
training epochs started 2020-12-23-05-48-55
Evaluate data in 2.39 seconds!
Evaluation on dev at Epoch 1/5. Step:179/895: 
SpanFPreRecMetric: f=0.698492, pre=0.673123, rec=0.725849
label_acc: acc=0.983848

Evaluate data in 2.39 seconds!
Evaluation on dev at Epoch 2/5. Step:358/895: 
SpanFPreRecMetric: f=0.733127, pre=0.697291, rec=0.772846
label_acc: acc=0.985342

Evaluate data in 2.4 seconds!
Evaluation on dev at Epoch 3/5. Step:537/895: 
SpanFPreRecMetric: f=0.744186, pre=0.717576, rec=0.772846
label_acc: acc=0.986155

Evaluate data in 2.4 seconds!
Evaluation on dev at Epoch 4/5. Step:716/895: 
SpanFPreRecMetric: f=0.731458, pre=0.716792, rec=0.746736
label_acc: acc=0.985711

Evaluate data in 2.39 seconds!
Evaluation on dev at Epoch 5/5. Step:895/895: 
SpanFPreRecMetric: f=0.778497, pre=0.772494, rec=0.784595
label_acc: acc=0.98867


In Epoch:5/Step:895, got best dev performance:
SpanFPreRecMetric: f=0.778497, pre=0.772494, rec=0.784595
label_acc: acc=0.98867
Reloaded the best model.
Evaluating...
              precision    recall  f1-score   support

          ID       0.50      1.00      0.67         1
     contact       0.69      0.73      0.71        15
   education       0.00      0.00      0.00         1
      family       0.69      0.75      0.72        12
    location       0.68      0.86      0.76        37
    med_exam       0.80      0.90      0.85       105
       money       0.75      0.96      0.84        25
        name       0.95      0.88      0.91        67
  profession       0.33      0.20      0.25         5
        time       0.76      0.74      0.75       498

   micro avg       0.77      0.78      0.78       766
   macro avg       0.62      0.70      0.65       766
weighted avg       0.77      0.78      0.78       766

=============================
Visualize validation....
predicting...
writing ../pred/bert/4.npy...
writing output.tsv file...
